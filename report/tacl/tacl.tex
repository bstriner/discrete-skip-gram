%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts}
 
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{url}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Instructions for TACL Submissions}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This document contains the instructions for preparing a manuscript for submission to TACL. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.
\end{abstract}

\section{Credits}

This document has been adapted from the instructions for ACL proceedings, including those for ACL-2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, those for ACL-2005 by Hwee Tou Ng and Kemal Oflazer, those for ACL-2002 by Eugene Charniak and Dekang Lin, and earlier ACL and EACL formats. Those versions were written by several people, including John Chen, Henry S. Thompson and Donald Walker. Additional elements were taken from the formatting instructions of the {\em International Joint Conference on Artificial Intelligence}.

\section{Introduction}

placeholder
\section{Factoring a joint distribution}

Let $A$ be a joint distribution matrix of two discrete variables $X$ and $Y$.

$$A_{i,j}=P(X=i, Y=j)$$

The entropy of the marginal distribution ($H(Y)$) is the best performance of a model that has no information regarding $X$. In the case of a skipgram covariance matrix, this is the entropy of the unigram distribution.

\begin{align*}
f(x) &= -x \log(x) \\
P(Y=j) &= \sum^i A_{i, j} \\
H(Y) &=  \sum^j f(P(Y=j))
\end{align*}

The conditional entropy ($H(Y \mid X)$) is the best performance of a model that is unconstrained, for instance, a skipgram model with a large number of hidden units. The conditional probability is the joint probability times the log of the conditional probability, or alternatively the marginal probability times the entropy of the conditional distribution.

\setlength{\thinmuskip}{0mu}
\setlength{\medmuskip}{0mu}
\setlength{\thickmuskip}{0mu} 
$$H(Y \mid X)=\sum^{i,j} - P(X=i, Y=j) \log(P(Y=j \mid X=i)) $$
$$H(Y \mid X)= \sum^{i,j}  P(X=i) f(P(Y=j \mid X=i))$$

The function $f(x)=-x \log(x)$ is convex, therefore the entropy of a linear combination of distributions is greater than or equal to the linear combination of the entropy of the distributions. The entropy of the marginal distribution is greater than or equal to the conditional entropy.

\subsection{Continuous factoring of a conditional distribution}

The most common method of continuous factoring is to embed each word into some continuous representation. The actual conditional distribution is approximated ($\tilde{P}$) by the output of a neural network. The loss ($L$) is the expected negative log conditional likelihood. See, for example, \cite{DBLP:journals/corr/MikolovSCCD13} and \cite{DBLP:journals/corr/abs-1301-3781}. 

A simple version of this model is parameterized by embeddings ($x$), weights ($W$) and bias ($b$) and uses a softmax to squash the outputs.

$$ \tilde{P}(Y\mid X=i) = \operatorname{softmax}(x_i W+b)$$
$$ L(x,W,b) = -\sum^{i, j} P(Y=j,X=i) \log \tilde{P}(Y=j \mid X=i) $$
$$ L(x,W,b) = -\mathbb{E}_{[i \sim X, j \sim Y]} \log \tilde{P}(Y=j \mid X=i) $$

\subsection{Discrete factoring or clustering}

A common and relatively efficient practice for producing discrete clusters is to train a neural network and then apply clustering techniques to the learned embeddings. In this paper, we examine how much information is actually retained by applying clustering to learned embeddings. We also examine methods to directly cluster with respect to a given loss function.

If examples in a dataset are clustered, the output that minimizes the loss for those examples may be calculated analytically or iteratively. In the case of a conditional distribution, the output that minimizes the loss for a cluster of examples is the combined distribution. The expected negative log conditional likelihood is the conditional entropy of the combined distributions. Similar approaches could be applied to more complicated neural networks, but discrete factorization of a conditional distribution is highly efficient and works well for analysis of the core optimization problem.

Let $C$ be an indicator matrix where $C_{j,i}$ indicates that $X=i$ should be encoded as $Z=j$. The joint distribution matrix $B_{i,j}=P(Y=j,Z=i)$ is efficiently calculated using the dot product. $C$ is constrained to all $0$ except one and only one $1$ per column.

$$B = C \cdot A $$ 

If the inputs $X$ are encoded discretely into $Z$ where $Z$ is overfull, some information is lost.  The covariance matrix $B$ is a linear combination of rows of the matrix $A$ and therefore $H(Y|Z) \ge H(Y|X)$. 

By relaxing the constraints on $C$, we can create a parameterization that allows for gradient descent using the conditional entropy of $B$ as the loss. A simple parameterization
is $C=\operatorname{softmax}(W)$. If the softmax is fully saturated, each row will be assigned to one and only one cluster. At initialization, each row is assigned to a random mixture of clusters. By the convexity of $f$, we predict that the model will tend towards saturation. A mixture of distributions will have a higher entropy than the component distributions.

\subsection{Hierarchical clustering}

The simplest discrete structured encoding is a binary tree. The address of a node in a binary tree of depth $d$ may be represented as a vector of length $d$ containing only zeros and ones. A complete binary tree will contain $2^d$ leaves at the base of the tree and will ideally achieve similar performance to models using $2^d$ flat clusters.

A reasonable loss for hierarchical clusters is the weighted sum of the conditional entropy of the clusters at each depth in the tree. In the spirit of the Bellman equation, an exponential series is a reasonable choice of weighting. Let $C_d \in [0,1]^{(2^d, \lvert Y \rvert)}$ be the cluster membership at each depth in the tree.

$$ L = \sum_d \beta ^ d  H(Y \mid C_d) $$

Intuitively, one is optimizing a weighted sum of how much information is available given the depth ($d$) of the tree or how many bits of the representation are provided. The parameter $\beta$ controls the balance of having more information available early or better accuracy given more of the tree. This model attempts to pack more information into the early bits and remaining differentiation into the later bits. The meaning of the later bits is dependent on the earlier bits but the converse is not true.

We parameterize clusters at at the bottom layer of the tree $C_d$. We can then infer the membership of clusters at previous layers $C_0,...,C_{d-1}$ by summing leaves into parent nodes. We can then calculate the weighted loss over tree depth as described above.

\bibliography{tacl}
\bibliographystyle{acl2012}

\end{document}



