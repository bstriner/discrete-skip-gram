\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain}
\usepackage{booktabs} % For formal tables
\usepackage{csquotes}

\begin{document}
\title{Learning Discrete Structured Hidden Representations}
\date{May 14, 2017}
\subtitle{11-727 Final Report}

\author{Ben Striner}
\email{bstriner@andrew.cmu.edu}


\begin{abstract}
Machine-understandable representations are not frequently human-understandable representations. A common method to attempt to understand dense representations is postmortem clustering of those representations into a structured discrete space such as a tree or ontology. It is especially desirable to learn discrete embeddings of semantic information because traditional semantic resources such as Wordnet and Propbank have tree-like structures. I compare the information content of dense embeddings to the information content of postmortem clustering of those embeddings. I also propose two methods for learning discrete embeddings that show improved performance compared to postmortem clustering. The proposed methods produce hierarchies with better predictive power than hierarchies produced by postmortem clustering, so should provide more meaningful clustering when applied to various semantic tasks. Comparisons between models are provided using a simple skipgram objective but architectures could be used to place a discrete, structured bottleneck into any neural network. Clustering methods do not adequately capture the information contained in a dense embedding and additional architectures are required to represent information in a structured discrete space. Further analysis of the class of architectures that learn discrete structured embeddings would provide a tool to use for many semantic tasks.
\end{abstract}

\maketitle

\section{Introduction}

Semantic knowledge is traditionally represented in a structured discrete space. Linguistic knowledge is very naturally represented using trees or networks. Exemplary ontologies include Wordnet, Penman Upper Model,  Dublin Core, OpenCyc, Freebase, and DBpedia. There are many differences between the ontologies but a common trait is some sort of symbolic tree or network structure. An ontology typically contains some sort of symbol set and some set of rules and relationships.

The natural way to encode information and rules in a neural network are dense activations and dense weights. However, human beings find it hard to understand or visualize dense embeddings so clustering or dimensionality reduction is used to make those embeddings understandable. It is very common in natural language processing to report various clusterings and visualizations of dense representations. I will collectively refer to methods of analyzing a dense embedding after training as postmortem methods.

I experimentally compared postmortem methods that impose structure on a dense embedding to methods that directly learn a structured representation. I found that learning structured representations provided higher-quality representations than postmortem methods. I compared methods using a skipgram objective but techniques could be applied to other tasks.

None of the tested methods perfectly compress information into a limited structured discrete representation. Further analysis of architectures that learn discrete embeddings is required to build better tools to apply to semantic tasks.

It is not an unreasonable assumption that some distance measure or primary component analysis on a dense embedding is related to the information content. However, this is not a perfect relationship and structured methods can help identify what distinguishing features are important.

Conventional ontologies are  structured, discrete and deterministic. Some are trees but others are networks or partially-ordered.

A node's location in a tree can be represented as a sequential series of symbols representing each branch taken from the root node. For simplicity, experiments were performed using a simple binary tree with a depth of 10 but more complicated structures are possible. Learned embeddings represent at most 10 bits of information. The fixed tree depth and width makes computations more efficient.

The meaning of a word cannot be completely represented in a discrete fashion. Many semantic and linguistic phenomena are lines in the sand. There are many blurry edge cases but that does not undermine the usefulness of categories for most situations. An ideal ontology should have the properties of being mostly discrete and deterministic while still allowing for some ambiguous or edge cases.

Human beings can both identify members of a category (a discrete task) and identify certain members as more or less prototypical (a continuous task).

The proposed models can be easily viewed as discrete trees by using $\operatorname{argmax}$ but are actually differentiable approximations to a discrete tree. This enables use of the embeddings as both a continuous differentiable value and a discrete value.

\section{Related Work}

This paper uses a simple skipgram objective to compare representation learning between several architectures. Word representations learned from simple models have been found to encode some amount of semantic information. \cite{DBLP:journals/corr/MikolovSCCD13} \cite{DBLP:journals/corr/abs-1301-3781}

More advanced objectives would cause different, possibly more useful, information to be learned. For example, dependency-based word embeddings provide an objective with arguably more semantics than a skipgram. \cite{Levy2014}

Word representations can be factored using matrix decomposition but here I focus on neural methods. \cite{pennington2014glove}

Word ontologies have been embedded into dense vectors, the inverse of the current task. \cite{Bordes:2011:LSE:2900423.2900470}

There exists a great deal of work related to sub-word embeddings, such as \cite{Li2015}, that could be combined with the current work.

Deep structured semantic model (DSSM) provides a continuous structured representation of semantics \cite{unsupervised-learning-of-word-semantic-embedding-using-the-deep-structured-semantic-model}. 

There are many improvements to the simple softmax. Hierarchical softmax \cite{Morin05hierarchicalprobabilistic} and negative sampling provide significant performance improvements. I used a vanilla softmax with uniform smoothing to ensure a simple and fair comparison between models.

SeqGAN provides a method for discrete inference using reinforcement learning techniques \cite{DBLP:journals/corr/YuZWY16}. Experiments in the current paper relate to differentiable approximations to discrete inference, as actual discrete inference requires RL or other methods. However, it may be the case that some types of inference can only be performed using RL methods.

\section{Factoring a Covariance Matrix}

Let $A$ be a covariance matrix of two discrete variables $X$ and $Y$ where $A_{i,j}=P(X=i, Y=j)$.

The marginal distribution $P(Y=j)$ is $\sum^i A_{i, j}$. The entropy of that distribution is $\sum^j -P(Y=j)log(P(Y=j)$. 

The conditional entropy $H(Y \mid X)# is $\sum^{i,j} - P(X=i, Y=j) log(P(Y=j \mid X=i))$, the joint probability times the log of the conditional probability. The conditional entropy can equivalently be written as $\sum^i P(X=i) \sum^j P(Y=j, X=i) log(P(Y=j \mid X=i))$, the marginal probability times the entropy of the conditional distribution. 

The function $f(x)=-x \log(x)$ is convex, therefore the entropy of a linear combination of distributions is greater than or equal to the linear combination of the entropy of the distributions. The entropy of the marginal distribution is greater than or equal to the conditional entropy.

The entropy of the marginal distribution is the best performance of a model that has no information regarding $X$. In the case of a skipgram covariance matrix, this is the entropy of the unigram distribution. The conditional entropy is the best performance of a model that is unconstrained, for instance, a skipgram model with a large number of hidden units.

Let $C$ be an indicator matrix where $C_{j,i}$ indicates that $X=i$ should be encoded as $Z=j$. The covariance matrix $P(Y,Z)$ is efficiently calculated using the dot product $P(Y,Z) = C \cdot P(Y,X)$. The constraint on the indicator matrix is that there is only one $1$ per $X$.

If the inputs $X$ are encoded discretely into $Z$ where $Z$ is overfull, some information is lost.  The covariance matrix $P(Y,Z)$ is a linear combination of rows of the matrix of $P(Y,X)$ and therefore $H(Y|Z) \ge H(Y|X)$. 

\section{Baseline Models}

\section{Future Work}

I present analysis of extreme dimensionality reduction, compressing embeddings to a sequence of a few bits. Further analysis of methods for embedding into a discrete space would enable new models for language and other processing.

Reduction to a discrete space is especially important for linguistics since so many phenomena are naturally described in terms of symbols.

The discrete embedding model could be expanded to perform various types of beam searching and backtracking. Although that would make the implementation difficult it could be engineered to more closely embody theories of reasoning. The current implementation is not ideal and could be expanded into a much more powerful approach.

The present model is not trained to distinguish word senses. Inference of discrete choices, such as word senses, is an important aspect of future work

More challenging will be inference of a rule set. Future work should strive to infer a symbol set and rule set that can be used for higher-level logic.

Current models could be improved by analyzing issues with recurrence and balancing the model to improve performance for the deeper bits.

It would also be useful to compare proposed models using more challenging objectives than a skipgram.

Experiments used a simple binary tree for simplicity and consistency with prior work. More complicated trees, networks, or partially-ordered trees could provide more expressive hidden representations.

It would also be valuable to compare performance on learned trees to those based on coding theory such as a Huffman tree.

The task of efficient inference of discrete structured hidden representations will not be easy, as shown by the large discretization gaps across experiments.

Further analysis should also include determining whether an example is prototypical, which could be operationalized by the amount of saturation of the softmax. Viewing all words in an ontology may be overwhelming but filtering for the most prototypical words could make it manageable.

\section{Implementation Details}

Models were built in Python using a combination of Keras \cite{chollet2015keras} and Theano \cite{2016arXiv160502688short}.

Preprocessing consisted of removing punctuation and non-ASCII symbols, down-casing, and replacing words that occur less that 20 times with an unknown symbol.

All discrete models used a depth of 10 and symbol set of 2, so represent a total of 10 bits of information per word. After filtration and cleaning the corpus consisted of 4908 unique words plus the unknown symbol, so there should be an average of $\frac{4909}{2^{10}} \approx 4.8$ words per encoding.

Softmax activations in the prediction layers were uniformly smoothed by a factor of $10^{-8}$ to avoid numerical instability.

Recursive portions of the model were built using residual units \cite{DBLP:journals/corr/HeZRS15} in which each unit learns only a residual change to the hidden state,
$h_{i+1} = h_i + f(h_i, x_i)$ where $f$ is a multilayer network.

Models used LeakyReLU units for all internal activations \cite{Maas2013} \cite{DBLP:journals/corr/XuWCL15}.

A window of size 2 was used for context, providing a reasonable difference between the skipgram and unigram models. At large context sizes the performance of a skipgram model approaches a unigram model.

For further details, please refer to the source code, available on Github\footnote{\url{https://github.com/bstriner/discrete-skip-gram}}.

\nocite{*}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bstriner_report} 

\end{document}
